{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ClLAlkaXz2B"
      },
      "source": [
        "Импорты\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0cVSPk9X3Nd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.color_palette(\"bright\")\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn  import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "use_cuda = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhbNtiF8X41e"
      },
      "source": [
        "ОДЕ решатель (метод Эйлера)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzD-D6_lX-hF"
      },
      "outputs": [],
      "source": [
        "def ode_solve(z0, t0, t1, f):\n",
        "    h_max = 0.05\n",
        "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
        "\n",
        "    h = (t1 - t0)/n_steps\n",
        "    t = t0\n",
        "    z = z0\n",
        "\n",
        "    for i_step in range(n_steps):\n",
        "        z = z + h * f(z, t)\n",
        "        t = t + h\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-pAs6b1YAA3"
      },
      "source": [
        "Метод сопряженного уравнения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8ikWhVvYJAf"
      },
      "outputs": [],
      "source": [
        "class ODEAdjoint(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, z0, t, flat_parameters, func):\n",
        "        assert isinstance(func, ODEF)\n",
        "        bs, *z_shape = z0.size()\n",
        "        time_len = t.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n",
        "            z[0] = z0\n",
        "            for i_t in range(time_len - 1):\n",
        "                z0 = ode_solve(z0, t[i_t], t[i_t+1], func)\n",
        "                z[i_t+1] = z0\n",
        "\n",
        "        ctx.func = func\n",
        "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dLdz):\n",
        "        func = ctx.func\n",
        "        t, z, flat_parameters = ctx.saved_tensors\n",
        "        time_len, bs, *z_shape = z.size()\n",
        "        n_dim = np.prod(z_shape)\n",
        "        n_params = flat_parameters.size(0)\n",
        "\n",
        "        def augmented_dynamics(aug_z_i, t_i):\n",
        "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]\n",
        "            z_i = z_i.view(bs, *z_shape)\n",
        "            a = a.view(bs, *z_shape)\n",
        "            with torch.set_grad_enabled(True):\n",
        "                t_i = t_i.detach().requires_grad_(True)\n",
        "                z_i = z_i.detach().requires_grad_(True)\n",
        "\n",
        "                faug = func.forward_with_grad(z_i, t_i, grad_outputs=a)\n",
        "                func_eval, adfdz, adfdt, adfdp = faug\n",
        "\n",
        "                adfdz = adfdz if adfdz is not None else torch.zeros(bs, *z_shape)\n",
        "                adfdp = adfdp if adfdp is not None else torch.zeros(bs, n_params)\n",
        "                adfdt = adfdt if adfdt is not None else torch.zeros(bs, 1)\n",
        "                adfdz = adfdz.to(z_i)\n",
        "                adfdp = adfdp.to(z_i)\n",
        "                adfdt = adfdt.to(z_i)\n",
        "\n",
        "            func_eval = func_eval.view(bs, n_dim)\n",
        "            adfdz = adfdz.view(bs, n_dim)\n",
        "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
        "\n",
        "        dLdz = dLdz.view(time_len, bs, n_dim)\n",
        "        with torch.no_grad():\n",
        "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
        "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
        "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
        "\n",
        "            for i_t in range(time_len-1, 0, -1):\n",
        "                z_i = z[i_t]\n",
        "                t_i = t[i_t]\n",
        "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
        "\n",
        "                dLdz_i = dLdz[i_t]\n",
        "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2),\n",
        "                                   f_i.unsqueeze(-1))[:, 0]\n",
        "\n",
        "                adj_z += dLdz_i\n",
        "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
        "\n",
        "                aug_z = torch.cat((\n",
        "                    z_i.view(bs, n_dim),\n",
        "                    adj_z, torch.zeros(bs, n_params).to(z),\n",
        "                    adj_t[i_t]),\n",
        "                    dim=-1\n",
        "                )\n",
        "\n",
        "                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
        "\n",
        "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
        "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
        "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
        "\n",
        "                del aug_z, aug_ans\n",
        "\n",
        "            dLdz_0 = dLdz[0]\n",
        "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2),\n",
        "                                f_i.unsqueeze(-1))[:, 0]\n",
        "\n",
        "            adj_z += dLdz_0\n",
        "            adj_t[0] = adj_t[0] - dLdt_0\n",
        "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i1rd3yqYKk2"
      },
      "source": [
        "NeuralODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuQAh9RsYORE"
      },
      "outputs": [],
      "source": [
        "class NeuralODE(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super(NeuralODE, self).__init__()\n",
        "        assert isinstance(func, ODEF)\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
        "        t = t.to(z0)\n",
        "        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func)\n",
        "        if return_whole_sequence:\n",
        "            return z\n",
        "        else:\n",
        "            return z[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csXd83JmYSDS"
      },
      "source": [
        "ODEF и NNODEF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2cegHjuYYEm"
      },
      "outputs": [],
      "source": [
        "class ODEF(nn.Module):\n",
        "    def forward_with_grad(self, z, t, grad_outputs):\n",
        "        batch_size = z.shape[0]\n",
        "\n",
        "        out = self.forward(z, t)\n",
        "\n",
        "        a = grad_outputs\n",
        "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
        "            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n",
        "            allow_unused=True, retain_graph=True\n",
        "        )\n",
        "        if adfdp is not None:\n",
        "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
        "            adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
        "        if adfdt is not None:\n",
        "            adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
        "        return out, adfdz, adfdt, adfdp\n",
        "\n",
        "    def flatten_parameters(self):\n",
        "        p_shapes = []\n",
        "        flat_parameters = []\n",
        "        for p in self.parameters():\n",
        "            p_shapes.append(p.size())\n",
        "            flat_parameters.append(p.flatten())\n",
        "        return torch.cat(flat_parameters)\n",
        "\n",
        "class NNODEF(ODEF):\n",
        "    def __init__(self, in_dim, hid_dim, time_invariant=False):\n",
        "        super(NNODEF, self).__init__()\n",
        "        self.time_invariant = time_invariant\n",
        "        if time_invariant:\n",
        "            self.lin1 = nn.Linear(in_dim, hid_dim)\n",
        "        else:\n",
        "            self.lin1 = nn.Linear(in_dim + 1, hid_dim)\n",
        "        self.lin2 = nn.Linear(hid_dim, hid_dim)\n",
        "        self.lin3 = nn.Linear(hid_dim, in_dim)\n",
        "        self.elu = nn.ELU(inplace=True)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        if not self.time_invariant:\n",
        "            x = torch.cat((x, t), dim=-1)\n",
        "        h = self.elu(self.lin1(x))\n",
        "        h = self.elu(self.lin2(h))\n",
        "        out = self.lin3(h)\n",
        "        return out\n",
        "\n",
        "def to_np(x):\n",
        "    return x.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gszf48vIYZWl"
      },
      "source": [
        "Энкодер и декодер"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHcuuYaf-c9R"
      },
      "outputs": [],
      "source": [
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.rnn = nn.GRU(input_dim + 1, hidden_dim)\n",
        "        self.hid2lat = nn.Linear(hidden_dim, 2 * latent_dim)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t = t.clone()\n",
        "        t = t.unsqueeze(-1)\n",
        "        t[1:] = t[:-1] - t[1:]\n",
        "        t[0] = 0.\n",
        "        xt = torch.cat((x, t), dim=-1)\n",
        "        _, h0 = self.rnn(xt.flip((0,)))\n",
        "        z0 = self.hid2lat(h0[0])\n",
        "        z0_mean = z0[:, :self.latent_dim]\n",
        "        z0_log_var = z0[:, self.latent_dim:]\n",
        "        return z0_mean, z0_log_var\n",
        "\n",
        "class NeuralODEDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, latent_dim):\n",
        "        super(NeuralODEDecoder, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        func = NNODEF(latent_dim, hidden_dim, time_invariant=True)\n",
        "        self.ode = NeuralODE(func)\n",
        "        self.l2h = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.h2o = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, z0, t):\n",
        "        zs = self.ode(z0, t, return_whole_sequence=True)\n",
        "        hs = self.l2h(zs)\n",
        "        xs = self.h2o(hs)\n",
        "        return xs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UXFkBR_YefP"
      },
      "source": [
        "Итоговая модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_RlmXRnOvmf"
      },
      "outputs": [],
      "source": [
        "class ODEVAE(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, latent_dim):\n",
        "        super(ODEVAE, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = RNNEncoder(output_dim, hidden_dim, latent_dim)\n",
        "        self.decoder = NeuralODEDecoder(output_dim, hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, t, MAP=False):\n",
        "        z_mean, z_log_var = self.encoder(x, t)\n",
        "        if MAP:\n",
        "            z = z_mean\n",
        "        else:\n",
        "            z = z_mean + torch.randn_like(z_mean) * torch.exp(0.5 * z_log_var)\n",
        "        x_p = self.decoder(z, t)\n",
        "        return x_p, z, z_mean, z_log_var\n",
        "\n",
        "    def generate_with_seed(self, seed_x, t):\n",
        "        seed_t_len = seed_x.shape[0]\n",
        "        z_mean, z_log_var = self.encoder(seed_x, t[:seed_t_len])\n",
        "        x_p = self.decoder(z_mean, t)\n",
        "        return x_p\n",
        "\n",
        "    def predict_future(self, seed_x, t_seed, t_future):\n",
        "      z_mean, z_log_var = self.encoder(seed_x, t_seed)\n",
        "      z = z_mean + torch.randn_like(z_mean) * torch.exp(0.5 * z_log_var)\n",
        "      x_future = self.decoder(z, t_future)\n",
        "      return x_future\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSZY-y2tYkHA"
      },
      "source": [
        "Тестирование sin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lqqJA54aVOn"
      },
      "outputs": [],
      "source": [
        "t_max = 10\n",
        "n_points = 500\n",
        "time_np = np.linspace(0, t_max, num=n_points)\n",
        "time = torch.from_numpy(time_np[:, None]).to(torch.float32)\n",
        "\n",
        "orig_traj = torch.sin(time).unsqueeze(-1)\n",
        "noise_std = 0.02\n",
        "samp_traj = orig_traj + torch.randn_like(orig_traj) * noise_std\n",
        "\n",
        "vae = ODEVAE(output_dim=1, hidden_dim=64, latent_dim=6)\n",
        "if use_cuda:\n",
        "    vae = vae.cuda()\n",
        "\n",
        "optim = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 150\n",
        "patience_counter = 0\n",
        "save_path = \"best_model.pth\"\n",
        "\n",
        "for epoch in range(3000):\n",
        "    optim.zero_grad()\n",
        "    if use_cuda:\n",
        "        samp_traj = samp_traj.cuda()\n",
        "        time = time.cuda()\n",
        "\n",
        "    x_p, z, z_mean, z_log_var = vae(samp_traj, time)\n",
        "    loss = ((samp_traj - x_p) ** 2).mean() + 0.1 * (-0.5 * torch.mean(\n",
        "        1 + z_log_var - z_mean.pow(2) - torch.exp(z_log_var)))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        patience_counter = 0\n",
        "\n",
        "        if use_cuda:\n",
        "            vae.cpu()\n",
        "        torch.save(vae.state_dict(), save_path)\n",
        "        if use_cuda:\n",
        "            vae.cuda()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if epoch == 499:\n",
        "      print(f\"New best model saved at epoch {epoch} with loss {best_loss}\")\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch}. No improvement for {patience} epochs.\")\n",
        "        print(f\"New best model saved at epoch {epoch} with loss {best_loss}\")\n",
        "        break\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    vae.load_state_dict(torch.load(save_path))\n",
        "    print(\"Best model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3wHGtfMYqHC"
      },
      "source": [
        "Восстановление"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mKq2qO5pgYi"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    generated_traj = vae.generate_with_seed(samp_traj[:50], time)\n",
        "\n",
        "plt.plot(to_np(time).squeeze(), to_np(orig_traj).squeeze(), label=\"Original\", color='blue')\n",
        "plt.plot(to_np(time).squeeze(), to_np(generated_traj).squeeze(), label=\"Generated\", color='red', linestyle='--')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_l_58m0YrZr"
      },
      "source": [
        "Предсказание"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtXl_anFRZkq"
      },
      "outputs": [],
      "source": [
        "future_time_np = np.linspace(0, t_max * 2, num=100)\n",
        "future_time = torch.from_numpy(future_time_np[:, None]).to(torch.float32)\n",
        "\n",
        "t_seed = time\n",
        "t_future = future_time\n",
        "\n",
        "if use_cuda:\n",
        "    seed_x = samp_traj.cuda()\n",
        "    t_seed = t_seed.cuda()\n",
        "    t_future = t_future.cuda()\n",
        "else:\n",
        "    seed_x = samp_traj.cpu()\n",
        "    t_seed = t_seed.cpu()\n",
        "    t_future = t_future.cpu()\n",
        "\n",
        "with torch.no_grad():\n",
        "    future_traj = vae.predict_future(seed_x, t_seed, t_future)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(to_np(time).squeeze(), to_np(orig_traj).squeeze(), label=\"Original\", color='blue')\n",
        "plt.plot(to_np(future_time).squeeze(), to_np(future_traj).squeeze(), label=\"Future Prediction\", color='green', linestyle='--')\n",
        "\n",
        "plt.axvline(x=t_seed[-1].item(), color='black', linestyle='--', label=\"Seed Boundary\")\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Future Prediction\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTIiGD9WYtJH"
      },
      "source": [
        "Тестирование cos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnFHY13Y6Rvm"
      },
      "outputs": [],
      "source": [
        "t_max = 10\n",
        "n_points = 500\n",
        "time_np = np.linspace(0, t_max, num=n_points)\n",
        "time = torch.from_numpy(time_np[:, None]).to(torch.float32)\n",
        "\n",
        "orig_traj = torch.cos(time).unsqueeze(-1)\n",
        "noise_std = 0.02\n",
        "samp_traj = orig_traj + torch.randn_like(orig_traj) * noise_std\n",
        "\n",
        "vae = ODEVAE(output_dim=1, hidden_dim=64, latent_dim=6)\n",
        "if use_cuda:\n",
        "    vae = vae.cuda()\n",
        "\n",
        "optim = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 150\n",
        "patience_counter = 0\n",
        "save_path = \"best_model.pth\"\n",
        "\n",
        "for epoch in range(3000):\n",
        "    optim.zero_grad()\n",
        "    if use_cuda:\n",
        "        samp_traj = samp_traj.cuda()\n",
        "        time = time.cuda()\n",
        "\n",
        "    x_p, z, z_mean, z_log_var = vae(samp_traj, time)\n",
        "    loss = ((samp_traj - x_p) ** 2).mean() + 0.1 * (-0.5 * torch.mean(\n",
        "        1 + z_log_var - z_mean.pow(2) - torch.exp(z_log_var)))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        patience_counter = 0\n",
        "\n",
        "        if use_cuda:\n",
        "            vae.cpu()\n",
        "        torch.save(vae.state_dict(), save_path)\n",
        "        if use_cuda:\n",
        "            vae.cuda()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if epoch == 499:\n",
        "      print(f\"New best model saved at epoch {epoch} with loss {best_loss}\")\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch}. No improvement for {patience} epochs.\")\n",
        "        print(f\"New best model saved at epoch {epoch} with loss {best_loss}\")\n",
        "        break\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    vae.load_state_dict(torch.load(save_path))\n",
        "    print(\"Best model loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNg-gAJpYztl"
      },
      "source": [
        "Восстановление"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1b7FPydYzKv"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    generated_traj = vae.generate_with_seed(samp_traj[:50], time)\n",
        "\n",
        "plt.plot(to_np(time).squeeze(), to_np(orig_traj).squeeze(), label=\"Original\", color='blue')\n",
        "plt.plot(to_np(time).squeeze(), to_np(generated_traj).squeeze(), label=\"Generated\", color='red', linestyle='--')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjSJi_2HY19J"
      },
      "source": [
        "Предсказание"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqrLQEzXY2-5"
      },
      "outputs": [],
      "source": [
        "\n",
        "future_time_np = np.linspace(0, t_max * 2, num=100)\n",
        "future_time = torch.from_numpy(future_time_np[:, None]).to(torch.float32)\n",
        "\n",
        "t_seed = time\n",
        "t_future = future_time\n",
        "\n",
        "if use_cuda:\n",
        "    seed_x = samp_traj.cuda()\n",
        "    t_seed = t_seed.cuda()\n",
        "    t_future = t_future.cuda()\n",
        "else:\n",
        "    seed_x = samp_traj.cpu()\n",
        "    t_seed = t_seed.cpu()\n",
        "    t_future = t_future.cpu()\n",
        "\n",
        "with torch.no_grad():\n",
        "    future_traj = vae.predict_future(seed_x, t_seed, t_future)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(to_np(time).squeeze(), to_np(orig_traj).squeeze(), label=\"Original\", color='blue')\n",
        "plt.plot(to_np(future_time).squeeze(), to_np(future_traj).squeeze(), label=\"Future Prediction\", color='green', linestyle='--')\n",
        "\n",
        "plt.axvline(x=t_seed[-1].item(), color='black', linestyle='--', label=\"Seed Boundary\")\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Future Prediction\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qCutBGHZAmF"
      },
      "source": [
        "Подбор гиперпараметров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUHMYc6tZDcn"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "t_max = 10\n",
        "n_points = 1500\n",
        "time_np = np.linspace(0, t_max, num=n_points)\n",
        "time = torch.from_numpy(time_np[:, None]).to(torch.float32)\n",
        "orig_traj = torch.sin(time).unsqueeze(-1)\n",
        "\n",
        "hyperparams_grid = {\n",
        "    \"hidden_dim\": [64, 128],\n",
        "    \"latent_dim\": [4, 6, 8],\n",
        "    \"learning_rate\": [1e-3, 1e-4],\n",
        "    \"noise_std\": [0.01, 0.02, 0.03],\n",
        "    \"time_invariant\": [True, False]\n",
        "}\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_params = {}\n",
        "save_path_base = \"best_model_epoch\"\n",
        "\n",
        "for hidden_dim, latent_dim, learning_rate, noise_std, time_invariant in product(*hyperparams_grid.values()):\n",
        "    print(f\"\\nОбучение с параметрами: hidden_dim={hidden_dim}, latent_dim={latent_dim}, \"\n",
        "          f\"lr={learning_rate}, noise_std={noise_std}, time_invariant={time_invariant}\")\n",
        "\n",
        "    samp_traj = orig_traj + torch.randn_like(orig_traj) * noise_std\n",
        "    vae = ODEVAE(output_dim=1, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "    if use_cuda:\n",
        "        vae = vae.cuda()\n",
        "\n",
        "    optim = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "    patience = 20\n",
        "    patience_counter = 0\n",
        "    best_epoch_loss = float('inf')\n",
        "\n",
        "    for epoch in range(100):\n",
        "        vae.train()\n",
        "        optim.zero_grad()\n",
        "        if use_cuda:\n",
        "            samp_traj = samp_traj.cuda()\n",
        "            time = time.cuda()\n",
        "\n",
        "        x_p, z, z_mean, z_log_var = vae(samp_traj, time)\n",
        "        loss_recon = criterion(x_p, samp_traj)\n",
        "        kl_loss = -0.5 * torch.mean(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
        "        loss = loss_recon + 0.1 * kl_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        if loss.item() < best_epoch_loss:\n",
        "            best_epoch_loss = loss.item()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "\n",
        "    print(f\"Лучшая потеря на этих параметрах: {best_epoch_loss}\")\n",
        "    if best_epoch_loss < best_loss:\n",
        "        best_loss = best_epoch_loss\n",
        "        best_params = {\n",
        "            \"hidden_dim\": hidden_dim,\n",
        "            \"latent_dim\": latent_dim,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"noise_std\": noise_std,\n",
        "            \"time_invariant\": time_invariant\n",
        "        }\n",
        "        torch.save(vae.state_dict(), f\"{save_path_base}_hid{hidden_dim}_lat{latent_dim}.pth\")\n",
        "\n",
        "print(\"\\nЛучшие гиперпараметры:\")\n",
        "\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4MrvuZ0ZEAU"
      },
      "source": [
        "Тестирование cos после подбора гиперпараметров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwLjrfUWtpxM"
      },
      "outputs": [],
      "source": [
        "t_max = 10\n",
        "n_points = 500\n",
        "time_np = np.linspace(0, t_max, num=n_points)\n",
        "time = torch.from_numpy(time_np[:, None]).to(torch.float32)\n",
        "\n",
        "orig_traj = torch.cos(time).unsqueeze(-1)\n",
        "noise_std = 0.02\n",
        "samp_traj = orig_traj + torch.randn_like(orig_traj) * noise_std\n",
        "\n",
        "vae = ODEVAE(output_dim=1, hidden_dim=64, latent_dim=4)\n",
        "if use_cuda:\n",
        "    vae = vae.cuda()\n",
        "\n",
        "optim = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 150\n",
        "patience_counter = 0\n",
        "save_path = \"best_model.pth\"\n",
        "\n",
        "for epoch in range(3000):\n",
        "    optim.zero_grad()\n",
        "    if use_cuda:\n",
        "        samp_traj = samp_traj.cuda()\n",
        "        time = time.cuda()\n",
        "\n",
        "    x_p, z, z_mean, z_log_var = vae(samp_traj, time)\n",
        "    loss = ((samp_traj - x_p) ** 2).mean() + 0.1 * (-0.5 * torch.mean(\n",
        "        1 + z_log_var - z_mean.pow(2) - torch.exp(z_log_var)))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        patience_counter = 0\n",
        "\n",
        "        if use_cuda:\n",
        "            vae.cpu()\n",
        "        torch.save(vae.state_dict(), save_path)\n",
        "        if use_cuda:\n",
        "            vae.cuda()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if epoch == 499:\n",
        "      print(f\"New best model saved at epoch {epoch} with loss {best_loss}\")\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch}. No improvement for {patience} epochs.\")\n",
        "        print(f\"New best model saved at epoch {epoch} with loss {best_loss}\")\n",
        "        break\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    vae.load_state_dict(torch.load(save_path))\n",
        "    print(\"Best model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1bbHhDPZKeO"
      },
      "source": [
        "Восстановление"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uGc1QrrZLme"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    generated_traj = vae.generate_with_seed(samp_traj[:50], time)\n",
        "\n",
        "plt.plot(to_np(time).squeeze(), to_np(orig_traj).squeeze(), label=\"Original\", color='blue')\n",
        "plt.plot(to_np(time).squeeze(), to_np(generated_traj).squeeze(), label=\"Generated\", color='red', linestyle='--')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K87xim50ZME8"
      },
      "source": [
        "Предсказание"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCK-I_G3ZNG3"
      },
      "outputs": [],
      "source": [
        "\n",
        "future_time_np = np.linspace(0, t_max * 2, num=100)\n",
        "future_time = torch.from_numpy(future_time_np[:, None]).to(torch.float32)\n",
        "\n",
        "t_seed = time\n",
        "t_future = future_time\n",
        "\n",
        "if use_cuda:\n",
        "    seed_x = samp_traj.cuda()\n",
        "    t_seed = t_seed.cuda()\n",
        "    t_future = t_future.cuda()\n",
        "else:\n",
        "    seed_x = samp_traj.cpu()\n",
        "    t_seed = t_seed.cpu()\n",
        "    t_future = t_future.cpu()\n",
        "\n",
        "with torch.no_grad():\n",
        "    future_traj = vae.predict_future(seed_x, t_seed, t_future)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(to_np(time).squeeze(), to_np(orig_traj).squeeze(), label=\"Original\", color='blue')\n",
        "plt.plot(to_np(future_time).squeeze(), to_np(future_traj).squeeze(), label=\"Future Prediction\", color='green', linestyle='--')\n",
        "\n",
        "plt.axvline(x=t_seed[-1].item(), color='black', linestyle='--', label=\"Seed Boundary\")\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Future Prediction\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57bAfaJZQkl"
      },
      "source": [
        "Тестирование sin после подбора гиперпараметров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GznswivzuL9I"
      },
      "outputs": [],
      "source": [
        "t_max = 10\n",
        "n_points = 500\n",
        "time_np = np.linspace(0, t_max, num=n_points)\n",
        "time = torch.from_numpy(time_np[:, None]).to(torch.float32)\n",
        "\n",
        "orig_traj = torch.sin(time).unsqueeze(-1)\n",
        "noise_std = 0.02\n",
        "samp_traj = orig_traj + torch.randn_like(orig_traj) * noise_std\n",
        "\n",
        "vae = ODEVAE(output_dim=1, hidden_dim=64, latent_dim=4)\n",
        "if use_cuda:\n",
        "    vae = vae.cuda()\n",
        "\n",
        "optim = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 150\n",
        "patience_counter = 0\n",
        "save_path = \"best_model.pth\"\n",
        "\n",
        "for epoch in range(3000):\n",
        "    optim.zero_grad()\n",
        "    if use_cuda:\n",
        "        samp_traj = samp_traj.cuda()\n",
        "        time = time.cuda()\n",
        "\n",
        "    x_p, z, z_mean, z_log_var = vae(samp_traj, time)\n",
        "    loss = ((samp_traj - x_p) ** 2).mean() + 0.1 * (-0.5 * torch.mean(\n",
        "        1 + z_log_var - z_mean.pow(2) - torch.exp(z_log_var)))\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        patience_counter = 0\n",
        "\n",
        "        if use_cuda:\n",
        "            vae.cpu()\n",
        "        torch.save(vae.state_dict(), save_path)\n",
        "        if use_cuda:\n",
        "            vae.cuda()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if epoch == 499:\n",
        "      print(f\"New best model saved at epoch {epoch} with loss {best_loss}\")\n",
        "    if patience_counter >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch}. No improvement for {patience} epochs.\")\n",
        "        print(f\"New best model saved at epoch {epoch} with loss {best_loss}\")\n",
        "        break\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    vae.load_state_dict(torch.load(save_path))\n",
        "    print(\"Best model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MByCCIYdZU61"
      },
      "source": [
        "Восстановление"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0s3TrGSxjnX"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    generated_traj = vae.generate_with_seed(samp_traj[:50], time)\n",
        "\n",
        "plt.plot(to_np(time).squeeze(), to_np(orig_traj).squeeze(), label=\"Original\", color='blue')\n",
        "plt.plot(to_np(time).squeeze(), to_np(generated_traj).squeeze(), label=\"Generated\", color='red', linestyle='--')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ8FfH9OZa01"
      },
      "source": [
        "Предсказание"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfzWSYJGZbit"
      },
      "outputs": [],
      "source": [
        "future_time_np = np.linspace(0, t_max * 2, num=100)\n",
        "future_time = torch.from_numpy(future_time_np[:, None]).to(torch.float32)\n",
        "\n",
        "t_seed = time\n",
        "t_future = future_time\n",
        "\n",
        "if use_cuda:\n",
        "    seed_x = samp_traj.cuda()\n",
        "    t_seed = t_seed.cuda()\n",
        "    t_future = t_future.cuda()\n",
        "else:\n",
        "    seed_x = samp_traj.cpu()\n",
        "    t_seed = t_seed.cpu()\n",
        "    t_future = t_future.cpu()\n",
        "\n",
        "with torch.no_grad():\n",
        "    future_traj = vae.predict_future(seed_x, t_seed, t_future)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(to_np(time).squeeze(), to_np(orig_traj).squeeze(), label=\"Original\", color='blue')\n",
        "plt.plot(to_np(future_time).squeeze(), to_np(future_traj).squeeze(), label=\"Future Prediction\", color='green', linestyle='--')\n",
        "\n",
        "plt.axvline(x=t_seed[-1].item(), color='black', linestyle='--', label=\"Seed Boundary\")\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Future Prediction\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
